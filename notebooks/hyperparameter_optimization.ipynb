{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Hyperparameter Optimization\n",
    "\n",
    "This notebook implements a three-stage Bayesian optimization process for tuning XGBoost hyperparameters using Optuna. The approach systematically explores the hyperparameter space to reduce FPR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the Optimization Process\n",
    "\n",
    "The hyperparameter optimization follows a sequential approach, progressing from fundamental to advanced parameters. We begin by fine-tuning the basic tree structure through depth, learning rate, and number of trees. This foundation is then enhanced by optimizing how data and features are sampled, along with controlling leaf node sizes. The process ends in adjusting regularization parameters to balance model complexity and prevent overfitting, using split thresholds and both L1 and L2 penalties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 1: Basic Parameter Optimization\n",
    "\n",
    "- Optimizes fundamental XGBoost parameters\n",
    "- Optuna with 50 trials\n",
    "- The goal is to maximize classification precision\n",
    "- Parameter ranges:\n",
    "  - max_depth: 3 to 10\n",
    "  - learning_rate: 0.01 to 0.3\n",
    "  - n_estimators: 50 to 200\n",
    "\n",
    "### Stage 2: Sampling Parameter Optimization\n",
    "\n",
    "- Builds upon best parameters from Stage 1\n",
    "- Optuna with 30 trials\n",
    "- Parameter ranges:\n",
    "  - subsample: 0.5 to 1.0\n",
    "  - colsample_bytree: 0.5 to 1.0\n",
    "  - min_child_weight: 1 to 10\n",
    "\n",
    "### Stage 3: Regularization Parameter Optimization\n",
    "\n",
    "- Builds upon best parameters from Stages 1 and 2\n",
    "- Optuna with 30 trials\n",
    "- Parameter ranges:\n",
    "  - gamma: 0 to 5\n",
    "  - reg_alpha: 0 to 5\n",
    "  - reg_lambda: 0 to 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 59837,
     "status": "ok",
     "timestamp": 1733672896598,
     "user": {
      "displayName": "Bartek Dmitruk",
      "userId": "12702711119721340647"
     },
     "user_tz": -60
    },
    "id": "u_x1KDQnggTJ",
    "outputId": "1c1d991c-2222-472e-a526-f2598307d1f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.1.3)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
      "Collecting optuna\n",
      "  Downloading optuna-4.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost) (2.23.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Collecting alembic>=1.5.0 (from optuna)\n",
      "  Downloading alembic-1.14.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting colorlog (from optuna)\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.36)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.6)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna)\n",
      "  Downloading Mako-1.3.8-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
      "Downloading optuna-4.1.0-py3-none-any.whl (364 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m364.4/364.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading alembic-1.14.0-py3-none-any.whl (233 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Downloading Mako-1.3.8-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
      "Successfully installed Mako-1.3.8 alembic-1.14.0 colorlog-6.9.0 optuna-4.1.0\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost scikit-learn pandas optuna\n",
    "\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4464553,
     "status": "ok",
     "timestamp": 1733677401162,
     "user": {
      "displayName": "Bartek Dmitruk",
      "userId": "12702711119721340647"
     },
     "user_tz": -60
    },
    "id": "Uo_9xlf3rYk5",
    "outputId": "0e288c1d-8f08-4708-e5cf-e93e7c32f777"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-08 15:49:07,328] A new study created in memory with name: no-name-b2e1a0a8-36b0-491d-831e-4f6e5c4f7ea5\n",
      "[I 2024-12-08 15:49:46,029] Trial 0 finished with value: 0.7671485181671038 and parameters: {'max_depth': 6, 'learning_rate': 0.22559782883684773, 'n_estimators': 168}. Best is trial 0 with value: 0.7671485181671038.\n",
      "[I 2024-12-08 15:50:14,937] Trial 1 finished with value: 0.7669810794217782 and parameters: {'max_depth': 7, 'learning_rate': 0.20644201623785383, 'n_estimators': 105}. Best is trial 0 with value: 0.7671485181671038.\n",
      "[I 2024-12-08 15:51:12,065] Trial 2 finished with value: 0.7710554222247028 and parameters: {'max_depth': 10, 'learning_rate': 0.14281253195860238, 'n_estimators': 111}. Best is trial 2 with value: 0.7710554222247028.\n",
      "[I 2024-12-08 15:52:12,989] Trial 3 finished with value: 0.7683764022994921 and parameters: {'max_depth': 8, 'learning_rate': 0.07019434996598799, 'n_estimators': 180}. Best is trial 2 with value: 0.7710554222247028.\n",
      "[I 2024-12-08 15:52:47,764] Trial 4 finished with value: 0.7670927052519954 and parameters: {'max_depth': 8, 'learning_rate': 0.2183057869294654, 'n_estimators': 70}. Best is trial 2 with value: 0.7710554222247028.\n",
      "[I 2024-12-08 15:53:34,786] Trial 5 finished with value: 0.7684880281297092 and parameters: {'max_depth': 9, 'learning_rate': 0.21638956464711182, 'n_estimators': 127}. Best is trial 2 with value: 0.7710554222247028.\n",
      "[I 2024-12-08 15:54:47,749] Trial 6 finished with value: 0.7620137299771167 and parameters: {'max_depth': 8, 'learning_rate': 0.030676453365724894, 'n_estimators': 189}. Best is trial 2 with value: 0.7710554222247028.\n",
      "[I 2024-12-08 15:55:16,005] Trial 7 finished with value: 0.7614556008260311 and parameters: {'max_depth': 5, 'learning_rate': 0.19103508465215152, 'n_estimators': 128}. Best is trial 2 with value: 0.7710554222247028.\n",
      "[I 2024-12-08 15:55:53,600] Trial 8 finished with value: 0.7507395211251884 and parameters: {'max_depth': 5, 'learning_rate': 0.06391298292110438, 'n_estimators': 178}. Best is trial 2 with value: 0.7710554222247028.\n",
      "[I 2024-12-08 15:56:48,202] Trial 9 finished with value: 0.7688229056203606 and parameters: {'max_depth': 8, 'learning_rate': 0.13545387085348004, 'n_estimators': 166}. Best is trial 2 with value: 0.7710554222247028.\n",
      "[I 2024-12-08 15:56:58,491] Trial 10 finished with value: 0.7338840207624044 and parameters: {'max_depth': 3, 'learning_rate': 0.2840490698423912, 'n_estimators': 51}. Best is trial 2 with value: 0.7710554222247028.\n",
      "[I 2024-12-08 15:57:58,591] Trial 11 finished with value: 0.7710554222247028 and parameters: {'max_depth': 10, 'learning_rate': 0.1393547031099341, 'n_estimators': 149}. Best is trial 2 with value: 0.7710554222247028.\n",
      "[I 2024-12-08 15:58:44,450] Trial 12 finished with value: 0.7721158676117653 and parameters: {'max_depth': 10, 'learning_rate': 0.12784529364202274, 'n_estimators': 106}. Best is trial 12 with value: 0.7721158676117653.\n",
      "[I 2024-12-08 15:59:28,675] Trial 13 finished with value: 0.7708879834793771 and parameters: {'max_depth': 10, 'learning_rate': 0.11197488772739436, 'n_estimators': 96}. Best is trial 12 with value: 0.7721158676117653.\n",
      "[I 2024-12-08 16:00:11,637] Trial 14 finished with value: 0.7712228609700285 and parameters: {'max_depth': 10, 'learning_rate': 0.10954104361397313, 'n_estimators': 92}. Best is trial 12 with value: 0.7721158676117653.\n",
      "[I 2024-12-08 16:00:50,266] Trial 15 finished with value: 0.768767092705252 and parameters: {'max_depth': 9, 'learning_rate': 0.0918103277007206, 'n_estimators': 87}. Best is trial 12 with value: 0.7721158676117653.\n",
      "[I 2024-12-08 16:01:19,324] Trial 16 finished with value: 0.7699949768376402 and parameters: {'max_depth': 9, 'learning_rate': 0.1774193415088866, 'n_estimators': 78}. Best is trial 12 with value: 0.7721158676117653.\n",
      "[I 2024-12-08 16:01:53,612] Trial 17 finished with value: 0.7610090975051627 and parameters: {'max_depth': 10, 'learning_rate': 0.02114418248793408, 'n_estimators': 60}. Best is trial 12 with value: 0.7721158676117653.\n",
      "[I 2024-12-08 16:02:18,275] Trial 18 finished with value: 0.7345537757437071 and parameters: {'max_depth': 3, 'learning_rate': 0.1122052042926931, 'n_estimators': 141}. Best is trial 12 with value: 0.7721158676117653.\n",
      "[I 2024-12-08 16:02:48,875] Trial 19 finished with value: 0.7656415694591728 and parameters: {'max_depth': 7, 'learning_rate': 0.2635989349733534, 'n_estimators': 112}. Best is trial 12 with value: 0.7721158676117653.\n",
      "[I 2024-12-08 16:03:22,531] Trial 20 finished with value: 0.7704414801585087 and parameters: {'max_depth': 9, 'learning_rate': 0.16922850513046322, 'n_estimators': 94}. Best is trial 12 with value: 0.7721158676117653.\n",
      "[I 2024-12-08 16:04:11,714] Trial 21 finished with value: 0.7723391192721996 and parameters: {'max_depth': 10, 'learning_rate': 0.1493956897105445, 'n_estimators': 114}. Best is trial 21 with value: 0.7723391192721996.\n",
      "[I 2024-12-08 16:05:00,135] Trial 22 finished with value: 0.772785622593068 and parameters: {'max_depth': 10, 'learning_rate': 0.10846417402017274, 'n_estimators': 116}. Best is trial 22 with value: 0.772785622593068.\n",
      "[I 2024-12-08 16:05:57,644] Trial 23 finished with value: 0.7699391639225317 and parameters: {'max_depth': 9, 'learning_rate': 0.06606588131237907, 'n_estimators': 143}. Best is trial 22 with value: 0.772785622593068.\n",
      "[I 2024-12-08 16:06:47,140] Trial 24 finished with value: 0.77077635764916 and parameters: {'max_depth': 10, 'learning_rate': 0.15941137922940934, 'n_estimators': 120}. Best is trial 22 with value: 0.772785622593068.\n",
      "[I 2024-12-08 16:07:38,001] Trial 25 finished with value: 0.7706089189038343 and parameters: {'max_depth': 9, 'learning_rate': 0.09655846157647441, 'n_estimators': 134}. Best is trial 22 with value: 0.772785622593068.\n",
      "[I 2024-12-08 16:08:42,103] Trial 26 finished with value: 0.7706089189038343 and parameters: {'max_depth': 10, 'learning_rate': 0.13080140808604854, 'n_estimators': 156}. Best is trial 22 with value: 0.772785622593068.\n",
      "[I 2024-12-08 16:09:03,952] Trial 27 finished with value: 0.7237260702126472 and parameters: {'max_depth': 4, 'learning_rate': 0.044312039859217306, 'n_estimators': 104}. Best is trial 22 with value: 0.772785622593068.\n",
      "[I 2024-12-08 16:09:38,323] Trial 28 finished with value: 0.7617904783166825 and parameters: {'max_depth': 7, 'learning_rate': 0.08535427792584535, 'n_estimators': 118}. Best is trial 22 with value: 0.772785622593068.\n",
      "[I 2024-12-08 16:09:59,868] Trial 29 finished with value: 0.758832393815929 and parameters: {'max_depth': 6, 'learning_rate': 0.18403766612037317, 'n_estimators': 79}. Best is trial 22 with value: 0.772785622593068.\n",
      "[I 2024-12-08 16:10:42,839] Trial 30 finished with value: 0.7682089635541665 and parameters: {'max_depth': 8, 'learning_rate': 0.15131049454375636, 'n_estimators': 133}. Best is trial 22 with value: 0.772785622593068.\n",
      "[I 2024-12-08 16:11:25,557] Trial 31 finished with value: 0.7711112351398114 and parameters: {'max_depth': 10, 'learning_rate': 0.11438097653887008, 'n_estimators': 94}. Best is trial 22 with value: 0.772785622593068.\n",
      "[I 2024-12-08 16:12:11,494] Trial 32 finished with value: 0.772673996762851 and parameters: {'max_depth': 10, 'learning_rate': 0.12247118079297795, 'n_estimators': 104}. Best is trial 22 with value: 0.772785622593068.\n",
      "[I 2024-12-08 16:12:52,002] Trial 33 finished with value: 0.7702182284980744 and parameters: {'max_depth': 9, 'learning_rate': 0.12278397961483813, 'n_estimators': 105}. Best is trial 22 with value: 0.772785622593068.\n",
      "[I 2024-12-08 16:13:37,608] Trial 34 finished with value: 0.7684880281297092 and parameters: {'max_depth': 10, 'learning_rate': 0.24498329765280868, 'n_estimators': 113}. Best is trial 22 with value: 0.772785622593068.\n",
      "[I 2024-12-08 16:14:14,539] Trial 35 finished with value: 0.7695484735167718 and parameters: {'max_depth': 9, 'learning_rate': 0.20100305374251504, 'n_estimators': 101}. Best is trial 22 with value: 0.772785622593068.\n",
      "[I 2024-12-08 16:15:04,312] Trial 36 finished with value: 0.7711112351398114 and parameters: {'max_depth': 10, 'learning_rate': 0.15757740978688653, 'n_estimators': 121}. Best is trial 22 with value: 0.772785622593068.\n",
      "[I 2024-12-08 16:15:35,650] Trial 37 finished with value: 0.762348607467768 and parameters: {'max_depth': 8, 'learning_rate': 0.07773791156737839, 'n_estimators': 82}. Best is trial 22 with value: 0.772785622593068.\n",
      "[I 2024-12-08 16:16:07,828] Trial 38 finished with value: 0.7622369816375509 and parameters: {'max_depth': 9, 'learning_rate': 0.05072765698323038, 'n_estimators': 69}. Best is trial 22 with value: 0.772785622593068.\n",
      "[I 2024-12-08 16:16:37,329] Trial 39 finished with value: 0.7571580063626723 and parameters: {'max_depth': 6, 'learning_rate': 0.09459285654562705, 'n_estimators': 114}. Best is trial 22 with value: 0.772785622593068.\n",
      "[I 2024-12-08 16:17:18,358] Trial 40 finished with value: 0.7670368923368868 and parameters: {'max_depth': 8, 'learning_rate': 0.14642131256155366, 'n_estimators': 128}. Best is trial 22 with value: 0.772785622593068.\n",
      "[I 2024-12-08 16:18:01,236] Trial 41 finished with value: 0.7712228609700285 and parameters: {'max_depth': 10, 'learning_rate': 0.1041348604190896, 'n_estimators': 89}. Best is trial 22 with value: 0.772785622593068.\n",
      "[I 2024-12-08 16:18:47,971] Trial 42 finished with value: 0.7719484288664397 and parameters: {'max_depth': 10, 'learning_rate': 0.120664781975172, 'n_estimators': 107}. Best is trial 22 with value: 0.772785622593068.\n",
      "[I 2024-12-08 16:19:35,886] Trial 43 finished with value: 0.7722833063570911 and parameters: {'max_depth': 10, 'learning_rate': 0.12447544890479258, 'n_estimators': 105}. Best is trial 22 with value: 0.772785622593068.\n",
      "[I 2024-12-08 16:20:11,961] Trial 44 finished with value: 0.7691577831110119 and parameters: {'max_depth': 9, 'learning_rate': 0.1293713027958154, 'n_estimators': 99}. Best is trial 22 with value: 0.772785622593068.\n",
      "[I 2024-12-08 16:21:33,560] Trial 45 finished with value: 0.7684322152146007 and parameters: {'max_depth': 10, 'learning_rate': 0.17029894960313866, 'n_estimators': 197}. Best is trial 22 with value: 0.772785622593068.\n",
      "[I 2024-12-08 16:22:14,317] Trial 46 finished with value: 0.7691019701959033 and parameters: {'max_depth': 9, 'learning_rate': 0.13494160416105438, 'n_estimators': 108}. Best is trial 22 with value: 0.772785622593068.\n",
      "[I 2024-12-08 16:23:04,539] Trial 47 finished with value: 0.7708879834793771 and parameters: {'max_depth': 10, 'learning_rate': 0.14730021141891106, 'n_estimators': 126}. Best is trial 22 with value: 0.772785622593068.\n",
      "[I 2024-12-08 16:23:34,205] Trial 48 finished with value: 0.7501813919741028 and parameters: {'max_depth': 5, 'learning_rate': 0.0766447978530539, 'n_estimators': 134}. Best is trial 22 with value: 0.772785622593068.\n",
      "[I 2024-12-08 16:24:04,638] Trial 49 finished with value: 0.771278673885137 and parameters: {'max_depth': 10, 'learning_rate': 0.19990489225899902, 'n_estimators': 72}. Best is trial 22 with value: 0.772785622593068.\n",
      "[I 2024-12-08 16:24:04,643] A new study created in memory with name: no-name-67092671-bb3b-482e-a50a-500d7b556e40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 Best Parameters: {'max_depth': 10, 'learning_rate': 0.10846417402017274, 'n_estimators': 116}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-08 16:24:44,578] Trial 0 finished with value: 0.7658648211196071 and parameters: {'subsample': 0.5880963925858063, 'colsample_bytree': 0.5504913614676639, 'min_child_weight': 5}. Best is trial 0 with value: 0.7658648211196071.\n",
      "[I 2024-12-08 16:25:19,723] Trial 1 finished with value: 0.7679298989786236 and parameters: {'subsample': 0.9770637161572737, 'colsample_bytree': 0.6286715543596686, 'min_child_weight': 10}. Best is trial 1 with value: 0.7679298989786236.\n",
      "[I 2024-12-08 16:25:57,839] Trial 2 finished with value: 0.7668136406764525 and parameters: {'subsample': 0.6938129915389537, 'colsample_bytree': 0.6701802255793936, 'min_child_weight': 7}. Best is trial 1 with value: 0.7679298989786236.\n",
      "[I 2024-12-08 16:26:44,558] Trial 3 finished with value: 0.7664229502706926 and parameters: {'subsample': 0.6323791594959572, 'colsample_bytree': 0.6988455293836077, 'min_child_weight': 2}. Best is trial 1 with value: 0.7679298989786236.\n",
      "[I 2024-12-08 16:27:24,090] Trial 4 finished with value: 0.7685996539599264 and parameters: {'subsample': 0.8296741472304574, 'colsample_bytree': 0.5681213870601967, 'min_child_weight': 5}. Best is trial 4 with value: 0.7685996539599264.\n",
      "[I 2024-12-08 16:27:59,736] Trial 5 finished with value: 0.7668694535915611 and parameters: {'subsample': 0.9840388028786038, 'colsample_bytree': 0.5999855923452821, 'min_child_weight': 10}. Best is trial 4 with value: 0.7685996539599264.\n",
      "[I 2024-12-08 16:28:35,247] Trial 6 finished with value: 0.7646927499023274 and parameters: {'subsample': 0.6541678156649243, 'colsample_bytree': 0.5380696879185978, 'min_child_weight': 9}. Best is trial 4 with value: 0.7685996539599264.\n",
      "[I 2024-12-08 16:29:18,416] Trial 7 finished with value: 0.7675950214879723 and parameters: {'subsample': 0.58428685198097, 'colsample_bytree': 0.6871074598441367, 'min_child_weight': 3}. Best is trial 4 with value: 0.7685996539599264.\n",
      "[I 2024-12-08 16:29:55,838] Trial 8 finished with value: 0.7664229502706926 and parameters: {'subsample': 0.823310657789443, 'colsample_bytree': 0.567526503711834, 'min_child_weight': 8}. Best is trial 4 with value: 0.7685996539599264.\n",
      "[I 2024-12-08 16:30:31,932] Trial 9 finished with value: 0.7679857118937322 and parameters: {'subsample': 0.7879329502291168, 'colsample_bytree': 0.5303037505424881, 'min_child_weight': 7}. Best is trial 4 with value: 0.7685996539599264.\n",
      "[I 2024-12-08 16:31:15,773] Trial 10 finished with value: 0.7696042864318804 and parameters: {'subsample': 0.871688943977605, 'colsample_bytree': 0.9149372657888759, 'min_child_weight': 4}. Best is trial 10 with value: 0.7696042864318804.\n",
      "[I 2024-12-08 16:31:59,738] Trial 11 finished with value: 0.7692135960261205 and parameters: {'subsample': 0.883276059694396, 'colsample_bytree': 0.8621089254469089, 'min_child_weight': 4}. Best is trial 10 with value: 0.7696042864318804.\n",
      "[I 2024-12-08 16:32:44,255] Trial 12 finished with value: 0.7704414801585087 and parameters: {'subsample': 0.8922772774495057, 'colsample_bytree': 0.9057966737290933, 'min_child_weight': 3}. Best is trial 12 with value: 0.7704414801585087.\n",
      "[I 2024-12-08 16:33:41,944] Trial 13 finished with value: 0.7705531059887258 and parameters: {'subsample': 0.9045808467540857, 'colsample_bytree': 0.9846755771262625, 'min_child_weight': 1}. Best is trial 13 with value: 0.7705531059887258.\n",
      "[I 2024-12-08 16:34:37,991] Trial 14 finished with value: 0.7711112351398114 and parameters: {'subsample': 0.9282393733736054, 'colsample_bytree': 0.9939219636333894, 'min_child_weight': 1}. Best is trial 14 with value: 0.7711112351398114.\n",
      "[I 2024-12-08 16:35:33,397] Trial 15 finished with value: 0.7709996093095942 and parameters: {'subsample': 0.935942000609402, 'colsample_bytree': 0.9962132484543237, 'min_child_weight': 1}. Best is trial 14 with value: 0.7711112351398114.\n",
      "[I 2024-12-08 16:36:30,067] Trial 16 finished with value: 0.769771725177206 and parameters: {'subsample': 0.7489441506110012, 'colsample_bytree': 0.9954073672108131, 'min_child_weight': 1}. Best is trial 14 with value: 0.7711112351398114.\n",
      "[I 2024-12-08 16:37:18,601] Trial 17 finished with value: 0.7646369369872188 and parameters: {'subsample': 0.500334963658843, 'colsample_bytree': 0.8034231643080679, 'min_child_weight': 2}. Best is trial 14 with value: 0.7711112351398114.\n",
      "[I 2024-12-08 16:38:11,675] Trial 18 finished with value: 0.7708321705642686 and parameters: {'subsample': 0.9470237885536359, 'colsample_bytree': 0.7946819991509717, 'min_child_weight': 1}. Best is trial 14 with value: 0.7711112351398114.\n",
      "[I 2024-12-08 16:38:59,708] Trial 19 finished with value: 0.7701066026678574 and parameters: {'subsample': 0.9488834495614581, 'colsample_bytree': 0.9045737727282994, 'min_child_weight': 2}. Best is trial 14 with value: 0.7711112351398114.\n",
      "[I 2024-12-08 16:39:45,922] Trial 20 finished with value: 0.7689903443656863 and parameters: {'subsample': 0.7451623799717454, 'colsample_bytree': 0.9493695815413251, 'min_child_weight': 3}. Best is trial 14 with value: 0.7711112351398114.\n",
      "[I 2024-12-08 16:40:38,145] Trial 21 finished with value: 0.7703298543282916 and parameters: {'subsample': 0.9327167580916058, 'colsample_bytree': 0.8007452339476363, 'min_child_weight': 1}. Best is trial 14 with value: 0.7711112351398114.\n",
      "[I 2024-12-08 16:41:27,487] Trial 22 finished with value: 0.7708321705642686 and parameters: {'subsample': 0.9933043002501178, 'colsample_bytree': 0.8387418797812831, 'min_child_weight': 1}. Best is trial 14 with value: 0.7711112351398114.\n",
      "[I 2024-12-08 16:42:13,122] Trial 23 finished with value: 0.7717251772060054 and parameters: {'subsample': 0.9347771906515, 'colsample_bytree': 0.7472126360381557, 'min_child_weight': 2}. Best is trial 23 with value: 0.7717251772060054.\n",
      "[I 2024-12-08 16:43:01,847] Trial 24 finished with value: 0.7702182284980744 and parameters: {'subsample': 0.8410499555120747, 'colsample_bytree': 0.754359805919884, 'min_child_weight': 2}. Best is trial 23 with value: 0.7717251772060054.\n",
      "[I 2024-12-08 16:43:40,785] Trial 25 finished with value: 0.7699391639225317 and parameters: {'subsample': 0.9185636131962968, 'colsample_bytree': 0.7488284492177786, 'min_child_weight': 4}. Best is trial 23 with value: 0.7717251772060054.\n",
      "[I 2024-12-08 16:44:27,930] Trial 26 finished with value: 0.7697159122620975 and parameters: {'subsample': 0.8550724871852994, 'colsample_bytree': 0.9481578752346069, 'min_child_weight': 3}. Best is trial 23 with value: 0.7717251772060054.\n",
      "[I 2024-12-08 16:45:13,627] Trial 27 finished with value: 0.7706089189038343 and parameters: {'subsample': 0.7933515557756732, 'colsample_bytree': 0.7431423686034374, 'min_child_weight': 2}. Best is trial 23 with value: 0.7717251772060054.\n",
      "[I 2024-12-08 16:45:54,681] Trial 28 finished with value: 0.7703298543282916 and parameters: {'subsample': 0.9536136680302506, 'colsample_bytree': 0.8602016014022648, 'min_child_weight': 6}. Best is trial 23 with value: 0.7717251772060054.\n",
      "[I 2024-12-08 16:46:44,896] Trial 29 finished with value: 0.7704972930736173 and parameters: {'subsample': 0.9157622889488952, 'colsample_bytree': 0.9584854980446285, 'min_child_weight': 1}. Best is trial 23 with value: 0.7717251772060054.\n",
      "[I 2024-12-08 16:46:44,900] A new study created in memory with name: no-name-918e1936-7762-44c0-993c-3449757d8a31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 Best Parameters: {'max_depth': 10, 'learning_rate': 0.10846417402017274, 'n_estimators': 116, 'subsample': 0.9347771906515, 'colsample_bytree': 0.7472126360381557, 'min_child_weight': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-08 16:47:09,429] Trial 0 finished with value: 0.7595021487972317 and parameters: {'gamma': 3.008395239220958, 'reg_alpha': 1.567899752338261, 'reg_lambda': 0.3819476557917728}. Best is trial 0 with value: 0.7595021487972317.\n",
      "[I 2024-12-08 16:47:30,166] Trial 1 finished with value: 0.7560417480605012 and parameters: {'gamma': 4.255064018359466, 'reg_alpha': 0.8114406264693924, 'reg_lambda': 2.9891188512468188}. Best is trial 0 with value: 0.7595021487972317.\n",
      "[I 2024-12-08 16:48:07,184] Trial 2 finished with value: 0.7699949768376402 and parameters: {'gamma': 0.59860752687388, 'reg_alpha': 0.8331702924238371, 'reg_lambda': 0.7750858785468784}. Best is trial 2 with value: 0.7699949768376402.\n",
      "[I 2024-12-08 16:48:36,646] Trial 3 finished with value: 0.7624044203828766 and parameters: {'gamma': 0.8316952336299183, 'reg_alpha': 4.013898837041079, 'reg_lambda': 2.9298814174916203}. Best is trial 2 with value: 0.7699949768376402.\n",
      "[I 2024-12-08 16:48:57,015] Trial 4 finished with value: 0.753306915220182 and parameters: {'gamma': 3.2587251879394143, 'reg_alpha': 4.190262153455912, 'reg_lambda': 2.1740955715275607}. Best is trial 2 with value: 0.7699949768376402.\n",
      "[I 2024-12-08 16:49:22,828] Trial 5 finished with value: 0.7595579617123402 and parameters: {'gamma': 1.6896087580927328, 'reg_alpha': 3.037818884809763, 'reg_lambda': 2.0771905802469566}. Best is trial 2 with value: 0.7699949768376402.\n",
      "[I 2024-12-08 16:49:49,834] Trial 6 finished with value: 0.7649718144778702 and parameters: {'gamma': 1.6633715697392852, 'reg_alpha': 1.1325267630399893, 'reg_lambda': 0.3807228974000598}. Best is trial 2 with value: 0.7699949768376402.\n",
      "[I 2024-12-08 16:50:10,459] Trial 7 finished with value: 0.7537534185410504 and parameters: {'gamma': 4.428868112294254, 'reg_alpha': 0.9473727947634492, 'reg_lambda': 4.5674045071273595}. Best is trial 2 with value: 0.7699949768376402.\n",
      "[I 2024-12-08 16:50:32,625] Trial 8 finished with value: 0.7525813473237707 and parameters: {'gamma': 4.257139875581208, 'reg_alpha': 2.8261104807408763, 'reg_lambda': 3.1682825330538495}. Best is trial 2 with value: 0.7699949768376402.\n",
      "[I 2024-12-08 16:50:54,521] Trial 9 finished with value: 0.7597254004576659 and parameters: {'gamma': 3.9532921337814466, 'reg_alpha': 1.1496097999659805, 'reg_lambda': 0.034528525903234075}. Best is trial 2 with value: 0.7699949768376402.\n",
      "[I 2024-12-08 16:51:38,786] Trial 10 finished with value: 0.7707205447340515 and parameters: {'gamma': 0.08059404460276698, 'reg_alpha': 0.20140487611042435, 'reg_lambda': 1.1956298861674801}. Best is trial 10 with value: 0.7707205447340515.\n",
      "[I 2024-12-08 16:52:19,286] Trial 11 finished with value: 0.7713902997153541 and parameters: {'gamma': 0.40485133721885, 'reg_alpha': 0.12792230585405645, 'reg_lambda': 1.3261593035347525}. Best is trial 11 with value: 0.7713902997153541.\n",
      "[I 2024-12-08 16:53:05,892] Trial 12 finished with value: 0.77077635764916 and parameters: {'gamma': 0.06097303016832312, 'reg_alpha': 0.027001822155836325, 'reg_lambda': 1.4122487915986133}. Best is trial 11 with value: 0.7713902997153541.\n",
      "[I 2024-12-08 16:53:52,881] Trial 13 finished with value: 0.7693810347714461 and parameters: {'gamma': 0.022352987556411012, 'reg_alpha': 0.15758616336136377, 'reg_lambda': 1.4729689960738532}. Best is trial 11 with value: 0.7713902997153541.\n",
      "[I 2024-12-08 16:54:17,435] Trial 14 finished with value: 0.7622927945526595 and parameters: {'gamma': 1.5018985316673454, 'reg_alpha': 2.0453764532807646, 'reg_lambda': 1.5075611505671076}. Best is trial 11 with value: 0.7713902997153541.\n",
      "[I 2024-12-08 16:54:53,261] Trial 15 finished with value: 0.7675950214879723 and parameters: {'gamma': 0.9004352217657474, 'reg_alpha': 0.026861936493737883, 'reg_lambda': 3.7596848437721286}. Best is trial 11 with value: 0.7713902997153541.\n",
      "[I 2024-12-08 16:55:15,816] Trial 16 finished with value: 0.758330077579952 and parameters: {'gamma': 2.0826319436737633, 'reg_alpha': 2.1331308954647294, 'reg_lambda': 1.9608797870272765}. Best is trial 11 with value: 0.7713902997153541.\n",
      "[I 2024-12-08 16:55:53,031] Trial 17 finished with value: 0.7655299436289558 and parameters: {'gamma': 0.5054860716015566, 'reg_alpha': 3.521629263746096, 'reg_lambda': 1.006875900507184}. Best is trial 11 with value: 0.7713902997153541.\n",
      "[I 2024-12-08 16:56:22,213] Trial 18 finished with value: 0.7643020594965675 and parameters: {'gamma': 1.0982571420339555, 'reg_alpha': 1.7339537672583296, 'reg_lambda': 2.5439679155596018}. Best is trial 11 with value: 0.7713902997153541.\n",
      "[I 2024-12-08 16:56:45,631] Trial 19 finished with value: 0.7624044203828766 and parameters: {'gamma': 2.4230181787571063, 'reg_alpha': 0.5658845058174249, 'reg_lambda': 1.6962425989148198}. Best is trial 11 with value: 0.7713902997153541.\n",
      "[I 2024-12-08 16:57:22,971] Trial 20 finished with value: 0.7658648211196071 and parameters: {'gamma': 0.34105960771945854, 'reg_alpha': 4.732537282579132, 'reg_lambda': 0.8175552986834909}. Best is trial 11 with value: 0.7713902997153541.\n",
      "[I 2024-12-08 16:58:07,507] Trial 21 finished with value: 0.7709996093095942 and parameters: {'gamma': 0.13681506168161353, 'reg_alpha': 0.2984273212666031, 'reg_lambda': 1.1986308040863167}. Best is trial 11 with value: 0.7713902997153541.\n",
      "[I 2024-12-08 16:58:37,690] Trial 22 finished with value: 0.7679857118937322 and parameters: {'gamma': 1.135663351875217, 'reg_alpha': 0.43773518579348913, 'reg_lambda': 1.154903445837824}. Best is trial 11 with value: 0.7713902997153541.\n",
      "[I 2024-12-08 16:59:20,095] Trial 23 finished with value: 0.7678740860635151 and parameters: {'gamma': 0.3284451254353644, 'reg_alpha': 1.400764165105841, 'reg_lambda': 2.5053546890806317}. Best is trial 11 with value: 0.7713902997153541.\n",
      "[I 2024-12-08 16:59:48,378] Trial 24 finished with value: 0.7678740860635151 and parameters: {'gamma': 1.3734887994408949, 'reg_alpha': 0.49318758250669886, 'reg_lambda': 0.5456118897711647}. Best is trial 11 with value: 0.7713902997153541.\n",
      "[I 2024-12-08 17:00:32,599] Trial 25 finished with value: 0.77077635764916 and parameters: {'gamma': 0.01821207461890212, 'reg_alpha': 0.07056784692617368, 'reg_lambda': 1.564033684979521}. Best is trial 11 with value: 0.7713902997153541.\n",
      "[I 2024-12-08 17:01:09,857] Trial 26 finished with value: 0.7705531059887258 and parameters: {'gamma': 0.6806688815257664, 'reg_alpha': 0.5411417633498341, 'reg_lambda': 0.037380334977500373}. Best is trial 11 with value: 0.7713902997153541.\n",
      "[I 2024-12-08 17:01:36,265] Trial 27 finished with value: 0.7620695428922253 and parameters: {'gamma': 1.9858661884419497, 'reg_alpha': 1.319466720986228, 'reg_lambda': 1.8124826907028018}. Best is trial 11 with value: 0.7713902997153541.\n",
      "[I 2024-12-08 17:02:14,758] Trial 28 finished with value: 0.7678740860635151 and parameters: {'gamma': 0.3551666290757143, 'reg_alpha': 1.8643967355157722, 'reg_lambda': 1.4051681450699667}. Best is trial 11 with value: 0.7713902997153541.\n",
      "[I 2024-12-08 17:02:38,583] Trial 29 finished with value: 0.7569347547022381 and parameters: {'gamma': 3.3826520780803953, 'reg_alpha': 2.376913944449724, 'reg_lambda': 0.5241100569070165}. Best is trial 11 with value: 0.7713902997153541.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 3 Best Parameters: {'max_depth': 10, 'learning_rate': 0.10846417402017274, 'n_estimators': 116, 'subsample': 0.9347771906515, 'colsample_bytree': 0.7472126360381557, 'min_child_weight': 2, 'gamma': 0.40485133721885, 'reg_alpha': 0.12792230585405645, 'reg_lambda': 1.3261593035347525}\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis       0.48      0.44      0.46        68\n",
      "      Backdoor       0.88      0.48      0.62        77\n",
      "           DoS       0.81      0.29      0.43       917\n",
      "      Exploits       0.81      0.78      0.80      6191\n",
      "       Fuzzers       0.69      0.95      0.80      5859\n",
      "       Generic       0.89      0.74      0.81       929\n",
      "Reconnaissance       0.90      0.67      0.77      3370\n",
      "     Shellcode       0.64      0.28      0.39       457\n",
      "         Worms       0.64      0.33      0.43        49\n",
      "\n",
      "      accuracy                           0.77     17917\n",
      "     macro avg       0.75      0.55      0.61     17917\n",
      "  weighted avg       0.79      0.77      0.76     17917\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    data = pd.read_csv(\"/content/drive/My Drive/data/Data.csv\")\n",
    "    labels = pd.read_csv(\"/content/drive/My Drive/data/Label.csv\")\n",
    "\n",
    "    df = pd.merge(data, labels, left_index=True, right_index=True)\n",
    "\n",
    "    X = df.drop(\"Label\", axis=1)\n",
    "    y = df[\"Label\"]\n",
    "\n",
    "    attack_mask = y > 0\n",
    "    X_attacks = X[attack_mask]\n",
    "    y_multiclass = y[attack_mask]\n",
    "\n",
    "    y_multiclass = y_multiclass - 1\n",
    "\n",
    "    categorical_cols = X.select_dtypes(include=[\"object\"]).columns\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col])\n",
    "        X_attacks[col] = le.fit_transform(X_attacks[col])\n",
    "\n",
    "    scaler_multi = StandardScaler()\n",
    "    X_attacks_scaled = scaler_multi.fit_transform(X_attacks)\n",
    "    X_attacks_scaled = pd.DataFrame(X_attacks_scaled, columns=X_attacks.columns)\n",
    "\n",
    "    return X_attacks_scaled, y_multiclass\n",
    "\n",
    "\n",
    "def create_directories():\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    multiclass_dir = f\"models/multiclass/xgboost_{timestamp}\"\n",
    "    os.makedirs(multiclass_dir, exist_ok=True)\n",
    "    return multiclass_dir\n",
    "\n",
    "\n",
    "def objective_stage1(trial, X_train, y_train, X_valid, y_valid):\n",
    "    param = {\n",
    "        'objective': 'multi:softmax',\n",
    "        'num_class': 9,\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
    "        'random_state': 42\n",
    "    }\n",
    "\n",
    "    model = xgb.XGBClassifier(**param)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    accuracy = (preds == y_valid).mean()\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def objective_stage2(trial, X_train, y_train, X_valid, y_valid, best_params):\n",
    "    param = {\n",
    "        **best_params,\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10)\n",
    "    }\n",
    "\n",
    "    model = xgb.XGBClassifier(**param)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    accuracy = (preds == y_valid).mean()\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def objective_stage3(trial, X_train, y_train, X_valid, y_valid, best_params):\n",
    "    param = {\n",
    "        **best_params,\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 5),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 5)\n",
    "    }\n",
    "\n",
    "    model = xgb.XGBClassifier(**param)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    accuracy = (preds == y_valid).mean()\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def train_and_evaluate():\n",
    "    X, y_multiclass = load_and_preprocess_data()\n",
    "    multiclass_dir = create_directories()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_multiclass, test_size=0.2, random_state=42)\n",
    "\n",
    "    study_stage1 = optuna.create_study(direction='maximize')\n",
    "    study_stage1.optimize(lambda trial: objective_stage1(trial, X_train, y_train, X_test, y_test), n_trials=50)\n",
    "    best_params_stage1 = study_stage1.best_params\n",
    "    print(\"Stage 1 Best Parameters:\", best_params_stage1)\n",
    "\n",
    "    study_stage2 = optuna.create_study(direction='maximize')\n",
    "    study_stage2.optimize(lambda trial: objective_stage2(trial, X_train, y_train, X_test, y_test, best_params_stage1), n_trials=30)\n",
    "    best_params_stage2 = {**best_params_stage1, **study_stage2.best_params}\n",
    "    print(\"Stage 2 Best Parameters:\", best_params_stage2)\n",
    "\n",
    "    study_stage3 = optuna.create_study(direction='maximize')\n",
    "    study_stage3.optimize(lambda trial: objective_stage3(trial, X_train, y_train, X_test, y_test, best_params_stage2), n_trials=30)\n",
    "    best_params_stage3 = {**best_params_stage2, **study_stage3.best_params}\n",
    "    print(\"Stage 3 Best Parameters:\", best_params_stage3)\n",
    "\n",
    "    final_model = xgb.XGBClassifier(**best_params_stage3)\n",
    "    final_model.fit(X_train, y_train)\n",
    "\n",
    "    preds = final_model.predict(X_test)\n",
    "    report = classification_report(y_test, preds, target_names=[\"Analysis\", \"Backdoor\", \"DoS\", \"Exploits\", \"Fuzzers\", \"Generic\", \"Reconnaissance\", \"Shellcode\", \"Worms\"])\n",
    "    print(report)\n",
    "\n",
    "    final_model.save_model(f\"{multiclass_dir}/final_model.json\")\n",
    "\n",
    "    with open(f\"{multiclass_dir}/report.txt\", \"w\") as f:\n",
    "        f.write(\"XGBoost Multiclass Classification Results\\n\")\n",
    "        f.write(\"==================================\\n\\n\")\n",
    "        f.write(f\"Stage 1 Best Parameters: {best_params_stage1}\\n\")\n",
    "        f.write(f\"Stage 2 Best Parameters: {best_params_stage2}\\n\")\n",
    "        f.write(f\"Stage 3 Best Parameters: {best_params_stage3}\\n\")\n",
    "        f.write(\"\\nClassification Report:\\n\")\n",
    "        f.write(report)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_and_evaluate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Stage 3 Best Parameters: {'max_depth': 10, 'learning_rate': 0.10846417402017274, 'n_estimators': 116, 'subsample': 0.9347771906515, 'colsample_bytree': 0.7472126360381557, 'min_child_weight': 2, 'gamma': 0.40485133721885, 'reg_alpha': 0.12792230585405645, 'reg_lambda': 1.3261593035347525}```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The optimization process produced a set of tuned parameters, with the final model using a relatively deep tree structure (max_depth: 10) balanced by a moderate learning rate (≈0.11). The sampling strategy favors using most of the training data (subsample: ≈0.93) while being more selective with features (colsample_bytree: ≈0.75). The regularization parameters suggest a light L1 penalty (reg_alpha: ≈0.13) with stronger L2 regularization (reg_lambda: ≈1.33).\n",
    "\n",
    "However, when comparing performance metrics, this optimized model shows only marginal improvements over XGBoost's default configuration. This suggests we've reached a performance plateau where:\n",
    "- The problem space may be well-covered by default parameters\n",
    "- The dataset's inherent patterns are already being effectively captured\n",
    "- Further parameter tuning is unlikely to yield meaningful gains"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPjTqV3vdO248JrustToru1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
